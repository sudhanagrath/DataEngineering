{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "--describe your project at a high level--\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import os\n",
    "from os.path import abspath\n",
    "import configparser\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import isnan, count, col, when,sum, countDistinct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read_file(open('aws_credentials.cfg'))\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('aws_credentials.cfg')\n",
    "access_id=config.get('AWS', 'AWS_ACCESS_KEY_ID')\n",
    "access_key=config.get('AWS', 'AWS_SECRET_ACCESS_KEY')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "Explain what you plan to do in the project in more detail. What data do you use? What is your end solution look like? What tools did you use? etc>\n",
    "\n",
    "#### Describe and Gather Data \n",
    "Describe the data sets you're using. Where did it come from? What type of information is included? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scope\n",
    "\n",
    "Scope of this Project is explore and describe the provided dataset, implement a data pipeline for it, using appropriate AWS Services that can handle the amount of data and answer questions when they scale up.\n",
    "\n",
    "### Describing and Gathering Data\n",
    "There are two datasets, i94 US immigration data and US cities and demographics data.\n",
    "\n",
    "I94 Immigration Data: This data comes from the US National Tourism and Trade Office. For more information, please visit below website: https://www.trade.gov/national-travel-and-tourism-office\n",
    "\n",
    "\n",
    "U.S. City Demographic Data: This data comes from OpenSoft. For more information, please visit below website: https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Median Age</th>\n",
       "      <th>Male Population</th>\n",
       "      <th>Female Population</th>\n",
       "      <th>Total Population</th>\n",
       "      <th>Number of Veterans</th>\n",
       "      <th>Foreign-born</th>\n",
       "      <th>Average Household Size</th>\n",
       "      <th>State Code</th>\n",
       "      <th>Race</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Silver Spring</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>33.8</td>\n",
       "      <td>40601.0</td>\n",
       "      <td>41862.0</td>\n",
       "      <td>82463</td>\n",
       "      <td>1562.0</td>\n",
       "      <td>30908.0</td>\n",
       "      <td>2.60</td>\n",
       "      <td>MD</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>25924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Quincy</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>41.0</td>\n",
       "      <td>44129.0</td>\n",
       "      <td>49500.0</td>\n",
       "      <td>93629</td>\n",
       "      <td>4147.0</td>\n",
       "      <td>32935.0</td>\n",
       "      <td>2.39</td>\n",
       "      <td>MA</td>\n",
       "      <td>White</td>\n",
       "      <td>58723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hoover</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>38.5</td>\n",
       "      <td>38040.0</td>\n",
       "      <td>46799.0</td>\n",
       "      <td>84839</td>\n",
       "      <td>4819.0</td>\n",
       "      <td>8229.0</td>\n",
       "      <td>2.58</td>\n",
       "      <td>AL</td>\n",
       "      <td>Asian</td>\n",
       "      <td>4759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rancho Cucamonga</td>\n",
       "      <td>California</td>\n",
       "      <td>34.5</td>\n",
       "      <td>88127.0</td>\n",
       "      <td>87105.0</td>\n",
       "      <td>175232</td>\n",
       "      <td>5821.0</td>\n",
       "      <td>33878.0</td>\n",
       "      <td>3.18</td>\n",
       "      <td>CA</td>\n",
       "      <td>Black or African-American</td>\n",
       "      <td>24437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Newark</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>34.6</td>\n",
       "      <td>138040.0</td>\n",
       "      <td>143873.0</td>\n",
       "      <td>281913</td>\n",
       "      <td>5829.0</td>\n",
       "      <td>86253.0</td>\n",
       "      <td>2.73</td>\n",
       "      <td>NJ</td>\n",
       "      <td>White</td>\n",
       "      <td>76402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Peoria</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>33.1</td>\n",
       "      <td>56229.0</td>\n",
       "      <td>62432.0</td>\n",
       "      <td>118661</td>\n",
       "      <td>6634.0</td>\n",
       "      <td>7517.0</td>\n",
       "      <td>2.40</td>\n",
       "      <td>IL</td>\n",
       "      <td>American Indian and Alaska Native</td>\n",
       "      <td>1343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Avondale</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>29.1</td>\n",
       "      <td>38712.0</td>\n",
       "      <td>41971.0</td>\n",
       "      <td>80683</td>\n",
       "      <td>4815.0</td>\n",
       "      <td>8355.0</td>\n",
       "      <td>3.18</td>\n",
       "      <td>AZ</td>\n",
       "      <td>Black or African-American</td>\n",
       "      <td>11592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>West Covina</td>\n",
       "      <td>California</td>\n",
       "      <td>39.8</td>\n",
       "      <td>51629.0</td>\n",
       "      <td>56860.0</td>\n",
       "      <td>108489</td>\n",
       "      <td>3800.0</td>\n",
       "      <td>37038.0</td>\n",
       "      <td>3.56</td>\n",
       "      <td>CA</td>\n",
       "      <td>Asian</td>\n",
       "      <td>32716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>O'Fallon</td>\n",
       "      <td>Missouri</td>\n",
       "      <td>36.0</td>\n",
       "      <td>41762.0</td>\n",
       "      <td>43270.0</td>\n",
       "      <td>85032</td>\n",
       "      <td>5783.0</td>\n",
       "      <td>3269.0</td>\n",
       "      <td>2.77</td>\n",
       "      <td>MO</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>2583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>High Point</td>\n",
       "      <td>North Carolina</td>\n",
       "      <td>35.5</td>\n",
       "      <td>51751.0</td>\n",
       "      <td>58077.0</td>\n",
       "      <td>109828</td>\n",
       "      <td>5204.0</td>\n",
       "      <td>16315.0</td>\n",
       "      <td>2.65</td>\n",
       "      <td>NC</td>\n",
       "      <td>Asian</td>\n",
       "      <td>11060</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               City           State  Median Age  Male Population  \\\n",
       "0     Silver Spring        Maryland        33.8          40601.0   \n",
       "1            Quincy   Massachusetts        41.0          44129.0   \n",
       "2            Hoover         Alabama        38.5          38040.0   \n",
       "3  Rancho Cucamonga      California        34.5          88127.0   \n",
       "4            Newark      New Jersey        34.6         138040.0   \n",
       "5            Peoria        Illinois        33.1          56229.0   \n",
       "6          Avondale         Arizona        29.1          38712.0   \n",
       "7       West Covina      California        39.8          51629.0   \n",
       "8          O'Fallon        Missouri        36.0          41762.0   \n",
       "9        High Point  North Carolina        35.5          51751.0   \n",
       "\n",
       "   Female Population  Total Population  Number of Veterans  Foreign-born  \\\n",
       "0            41862.0             82463              1562.0       30908.0   \n",
       "1            49500.0             93629              4147.0       32935.0   \n",
       "2            46799.0             84839              4819.0        8229.0   \n",
       "3            87105.0            175232              5821.0       33878.0   \n",
       "4           143873.0            281913              5829.0       86253.0   \n",
       "5            62432.0            118661              6634.0        7517.0   \n",
       "6            41971.0             80683              4815.0        8355.0   \n",
       "7            56860.0            108489              3800.0       37038.0   \n",
       "8            43270.0             85032              5783.0        3269.0   \n",
       "9            58077.0            109828              5204.0       16315.0   \n",
       "\n",
       "   Average Household Size State Code                               Race  Count  \n",
       "0                    2.60         MD                 Hispanic or Latino  25924  \n",
       "1                    2.39         MA                              White  58723  \n",
       "2                    2.58         AL                              Asian   4759  \n",
       "3                    3.18         CA          Black or African-American  24437  \n",
       "4                    2.73         NJ                              White  76402  \n",
       "5                    2.40         IL  American Indian and Alaska Native   1343  \n",
       "6                    3.18         AZ          Black or African-American  11592  \n",
       "7                    3.56         CA                              Asian  32716  \n",
       "8                    2.77         MO                 Hispanic or Latino   2583  \n",
       "9                    2.65         NC                              Asian  11060  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the data here\n",
    "# Reading U.S. City Demographic Data\n",
    "df1 =pd.read_csv(\"us-cities-demographics.csv\",sep=';')\n",
    "df1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading I94 Immigration Data\n",
    "#setting up environment to read sas_data folder\n",
    "os.environ[\"JAVA_HOME\"] = \"/Library/Internet Plug-Ins/JavaAppletPlugin.plugin/Contents/Home\"\n",
    "os.environ[\"PATH\"] = \"/Users/sudhanagrath/anaconda3/bin:/Users/sudhanagrath/anaconda3/condabin:/Users/sudhanagrath/spark-3.1.2-bin-hadoop2.7/bin:/Users/sudhanagrath/scala-2.13.6/bin:/Library/Frameworks/Python.framework/Versions/3.8/bin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/Applications/Postgres.app/Contents/Versions/latest/bin\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/Users/sudhanagrath/spark-3.1.2-bin-hadoop2.7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"CapstoneProject\")\\\n",
    "        .config(\"spark.sql.legacy.createHiveTableByDefault\",\"false\")\\\n",
    "        .config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11,org.apache.hadoop:hadoop-aws:2.7.4\")\\\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", access_id)\\\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\",access_key)\\\n",
    "        .enableHiveSupport().getOrCreate()\n",
    "sc=spark.sparkContext\n",
    "hadoop_conf=sc._jsc.hadoopConfiguration()\n",
    "hadoop_conf.set(\"fs.s3a.block.size\",\"32000000\")\n",
    "hadoop_conf.set(\"fs.s3a.multipart.size\",\"104857600\")\n",
    "hadoop_conf.set(\"fs.s3a.threads.core\",\"4\")\n",
    "hadoop_conf.set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading data in dataframe\n",
    "df_spark =spark.read.load('sas_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cicid: double (nullable = true)\n",
      " |-- i94yr: double (nullable = true)\n",
      " |-- i94mon: double (nullable = true)\n",
      " |-- i94cit: double (nullable = true)\n",
      " |-- i94res: double (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: double (nullable = true)\n",
      " |-- i94mode: double (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- depdate: double (nullable = true)\n",
      " |-- i94bir: double (nullable = true)\n",
      " |-- i94visa: double (nullable = true)\n",
      " |-- count: double (nullable = true)\n",
      " |-- dtadfile: string (nullable = true)\n",
      " |-- visapost: string (nullable = true)\n",
      " |-- occup: string (nullable = true)\n",
      " |-- entdepa: string (nullable = true)\n",
      " |-- entdepd: string (nullable = true)\n",
      " |-- entdepu: string (nullable = true)\n",
      " |-- matflag: string (nullable = true)\n",
      " |-- biryear: double (nullable = true)\n",
      " |-- dtaddto: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- insnum: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admnum: double (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#checking Schema\n",
    "df_spark.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "#### Cleaning Steps\n",
    "Document steps necessary to clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3096313"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# counting the total number of records in SAS Dataframe\n",
    "df_spark.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3096313"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# counting the distinct number of records in first column of the Dataframe\n",
    "df_spark.select(\"cicid\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify data quality issues\n",
    "# Duplicate records\n",
    "# Above two counts show that each record in the dataframe is identifed by its cicid, therefore there are no duplicate records in this dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-------+-------+-------+-------+-------+-------+-------+------+-------+-------+------+-----+--------+\n",
      "|cicid|i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|  occup|entdepa|entdepd|entdepu|matflag|biryear|dtaddto|gender| insnum|airline|admnum|fltno|visatype|\n",
      "+-----+-----+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-------+-------+-------+-------+-------+-------+-------+------+-------+-------+------+-----+--------+\n",
      "|    0|    0|     0|     0|     0|      0|      0|    239| 152592| 142457|   802|      0|    0|       1| 1881250|3088187|    238| 138429|3095921| 138429|    802|    477|414269|2982605|  83627|     0|19549|       0|\n",
      "+-----+-----+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-------+-------+-------+-------+-------+-------+-------+------+-------+-------+------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Identify data quality issues\n",
    "#finding missing or null values\n",
    "df1=df_spark.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df_spark.columns])\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#statistics shows that visapost, occup, entdepu,insnum have more than 50% of null or missing values. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying columns for dimensions\n",
    "df_spark.createOrReplaceTempView(\"sas_data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+\n",
      "|count(DISTINCT entdepa, entdepd)|\n",
      "+--------------------------------+\n",
      "|                              84|\n",
      "+--------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#entry_status dimension\n",
    "i94_dim1=df_spark.select(countDistinct(\"entdepa\",\"entdepd\"))\n",
    "i94_dim1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+\n",
      "|count(DISTINCT visatype)|\n",
      "+------------------------+\n",
      "|                      17|\n",
      "+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#visa_type dimension\n",
    "i94_dim2=df_spark.select(countDistinct(\"visatype\"))\n",
    "i94_dims2=spark.sql(\"select distinct visatype from sas_data\")\n",
    "i94_dim2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|count(DISTINCT i94visa)|\n",
      "+-----------------------+\n",
      "|                      3|\n",
      "+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#visa_code dimension\n",
    "i94_dim3=df_spark.select(countDistinct(\"i94visa\"))\n",
    "i94_dims3=spark.sql(\"select distinct i94visa from sas_data\")\n",
    "i94_dim3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|count(DISTINCT admnum)|\n",
      "+----------------------+\n",
      "|               3075579|\n",
      "+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#i94_visitor\n",
    "# This query shows that total number of distinct counts for admnum is approximately 20,000 less than the total number of records in the sas_data table\n",
    "i94_dim4=df_spark.select(countDistinct(\"admnum\"))\n",
    "\n",
    "i94_dim4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+-------+------+------+-------+------+-----+--------+\n",
      "|cicid|i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear|dtaddto|gender|insnum|airline|admnum|fltno|visatype|\n",
      "+-----+-----+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+-------+------+------+-------+------+-----+--------+\n",
      "+-----+-----+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+-------+------+------+-------+------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#This query shows that there are no Nulls and Nans in the sas_data table\n",
    "#i94_visitor=df_spark.filter(df_spark[\"admnum\"] ==''| df_spark[\"admnum\"].isNull()).show()\n",
    "i94_visitor=df_spark.filter(df_spark[\"admnum\"].isNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CODE</th>\n",
       "      <th>COUNTRY_CITY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>582</td>\n",
       "      <td>MEXICO Air Sea, and Not Reported (I-94, no lan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>236</td>\n",
       "      <td>AFGHANISTAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>101</td>\n",
       "      <td>ALBANIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>316</td>\n",
       "      <td>ALGERIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>102</td>\n",
       "      <td>ANDORRA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>759</td>\n",
       "      <td>INDIAN OCEAN AREAS (FRENCH)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>729</td>\n",
       "      <td>INDIAN OCEAN TERRITORY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>204</td>\n",
       "      <td>INDONESIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>249</td>\n",
       "      <td>IRAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>250</td>\n",
       "      <td>IRAQ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    CODE                                       COUNTRY_CITY\n",
       "0    582  MEXICO Air Sea, and Not Reported (I-94, no lan...\n",
       "1    236                                       AFGHANISTAN \n",
       "2    101                                           ALBANIA \n",
       "3    316                                           ALGERIA \n",
       "4    102                                           ANDORRA \n",
       "..   ...                                                ...\n",
       "95   759                       INDIAN OCEAN AREAS (FRENCH) \n",
       "96   729                            INDIAN OCEAN TERRITORY \n",
       "97   204                                         INDONESIA \n",
       "98   249                                              IRAN \n",
       "99   250                                              IRAQ \n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2=pd.read_csv(\"country_lookup.csv\", sep='\\t')\n",
    "df2.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df3=pd.read_csv(\"port-city-state.csv\", sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CODE</th>\n",
       "      <th>CITY_STATE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ALC</td>\n",
       "      <td>ALCAN, AK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ANC</td>\n",
       "      <td>ANCHORAGE, AK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BAR</td>\n",
       "      <td>BAKER AAF - BAKER ISLAND, AK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DAC</td>\n",
       "      <td>DALTONS CACHE, AK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PIZ</td>\n",
       "      <td>DEW STATION PT LAY DEW, AK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>APF</td>\n",
       "      <td>NAPLES, FL #ARPT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>OPF</td>\n",
       "      <td>OPA LOCKA, FL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>ORL</td>\n",
       "      <td>ORLANDO, FL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>PAN</td>\n",
       "      <td>PANAMA CITY, FL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>PEN</td>\n",
       "      <td>PENSACOLA, FL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    CODE                     CITY_STATE\n",
       "0   ALC                      ALCAN, AK \n",
       "1   ANC                  ANCHORAGE, AK \n",
       "2   BAR    BAKER AAF - BAKER ISLAND, AK\n",
       "3   DAC              DALTONS CACHE, AK \n",
       "4   PIZ      DEW STATION PT LAY DEW, AK\n",
       "..   ...                            ...\n",
       "95  APF                NAPLES, FL #ARPT\n",
       "96  OPF                   OPA LOCKA, FL\n",
       "97  ORL                    ORLANDO, FL \n",
       "98  PAN                PANAMA CITY, FL \n",
       "99  PEN                  PENSACOLA, FL \n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Map out the conceptual data model and explain why you chose that model\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Model\n",
    "This is a Star Schema data model for analysing the I94 Immigration Dataset.\n",
    "\n",
    "I94 Immigration dataset is considered as fact table. It has its associated 9 dimensions\n",
    "\n",
    "\n",
    "Please see the document CapstoneDataModel.docx\n",
    "\n",
    "### Mapping Pipeline to Data Model\n",
    "The data provided in this project comes in two formats, parquet and csv. Given the hive enabled, SAS module, \"saurfang:spark-sas7bdat:3.0.0-s_2.12\", pipeline is created in the following steps:\n",
    "- Ingest the sas_data dataset to AWS S3 bucket-capstone\n",
    "- Create HIVE enabled Spark session\n",
    "- Create internal HIVE table for each of the elevan tables in total for the data model.\n",
    "- Load data from the bucket-capstone into these tables which exist in the HIVE metastore, embedded derby database.This data is available only during one session\n",
    "- Create HIVE external tables for each of the internal HIVE tables. These tables are located in the AWS S3 bucket-hive. It requires creating elevan folders for each of the external tables in the bucket-hive.\n",
    "- Load data into the external tables.\n",
    "\n",
    "Note: HIVE metastore stores the metadata for the tables created based on data model. But the table data is located in AWS S3 bucket_hive. It is essential to create folders in the bucket-hive before creating and loading data into them.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline Datasets to Data Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three scripts to implement the pipeline\n",
    "#### sql_queries.py\n",
    "   This script works as module to import into the two scripts, create_tables.py and etl.py\n",
    "   \n",
    "   It defines:\n",
    "   \n",
    "   - list of queries for dropping the existing tables\n",
    "   - list of queries for creating the tables in data model\n",
    "   - list of queries for loading the data into tables in data model\n",
    "   - list of CSV schemas for loading the CSV data.\n",
    "   \n",
    "#### create_tables.py\n",
    "   It creates a spark session and runs the create table queries into that session. For details on type of tables, please see the readme.md\n",
    "   \n",
    "#### etl.py\n",
    "   It creates a spark session and runs the insert table queries into that session.\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform quality checks here\n",
    "- quality_checks.py need to be run after the etl.py has completed its run. It just counts the number of records loaded into HIVE external table.\n",
    "- unit_tests.py is another file that tests all the dimensions and fact tabls have unique key defined on each of them. This test has defined here because Spark SQL does not allow defining the constraints when creating tables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Please see the file data_dictionary.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5.1:The write up includes an outline of the steps taken in the project.\n",
    "##### Steps taken for this project with the given two datasets, SAS data and US Demograhics data, and the SAS module are:\n",
    " * Understand the data symantic with the help of relevant information available on the internet. Also spent quite a time on undestanding the given data type for the SAS module. \n",
    " * Tried using the Pandas first with the SAS data set, could not read it into data frame. Then \n",
    " used Spark dataframe. Setting up Spark session to load the data set required changes to default configurations such as envrionment with defined JAVA_HOME and SPARK_HOME.\n",
    " * Explored the two data sets as given briefly in the Step 2 of this document. Actually I have  tried to understand them in my local spark session well before the data model fits into my mind.\n",
    " * Put down the data model in the document submitted with this Project.\n",
    " * Given the Hive support in Spark session for the SAS module, I started reading about the data pipeline with this feature on the internet. \n",
    " * With my extensive experience in the data and databases, and extensive reading on Hive, mapped the data model to data pipeline as given in Step 3 of this document.\n",
    " * Implemented the data pipeline as I have learnt in this data engineering course.\n",
    " \n",
    " \n",
    "          \n",
    "##### The purpose of the final data model is made explicit.\n",
    "Purpose of the data model is to get insight on the trends of immigration, mainly with the SAS data set. This insight on immigration trend can be extended using US demographic data set. \n",
    "This is very basic data model and it can be used by anyone who is interested in knowing the Immigration Trend such as US visitors, travel and tourism related businesses etc.\n",
    "\n",
    "##### Types of queries which can be run on the implemented data pipeline are:\n",
    "\n",
    "*  Query1: List of top 5 countries visiting the United States\n",
    "*  Query2: International Arrivals to United States\n",
    "*  Query3: Count of vistitors using different Travel modes\n",
    "*  Query4: List of US Cities and States missing port of entries\n",
    "*  Query5: Count of visitors with specific visa type chosing their address in United States."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5.2:Propose how often the data should be updated and why\n",
    "Scripts for the data pipeline created in this project refreshes the data each time they run. But they can be modified to update the data on monthly basis. Because, we are working with immigration data in this project, I think schedule for monthly update should be correct when we would have the enough data for running the analytical queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5.3:Write a description of how you would approach the problem differently under the      following scenarios:\n",
    "##### The data was increased by 100x.\n",
    "In this project we have worked with data set of more than 3 million rows using local spark session. But if the data increases by hundred folds, my machine where I am running the spark session would not handle it and the pipeline need to to run on Spark cluster with multiple worker nodes, each having much larger computing capacity to process the data. As worked on a Spark project in this course, we will be using AWS's EMR service to create the Spark Cluster in case of increase in many folds of data.\n",
    "###### Is partitioning required ? If so how ? How can storage be handled ?\n",
    "Spark Framework can determine the partitioning automatically and it also allows its users to define the data paritioning programmatically.\n",
    "\n",
    "Automatic partitioning depends upon:\n",
    "Available Resources to the Spark job - Number of cores on which task can run on\n",
    "Data Sources - file format and size being used for processing\n",
    "RDD Paritioning - RDD is the low level data structure used by the spark to distribute data between tasks when data is being processed.So, depending upon the data set the partitions are housed on nodes of the spark cluser, but a parition does not span across the nodes. \n",
    "\n",
    "Users can define the partitioning by using pyspark method partitionBy() which divides the records depending upon the parition column(s) and puts each partition data into a sub-directory when a data frame is written to storge system.\n",
    "\n",
    "##### The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    "When there is a need to update the data based on daily basis, it also needs to be monitored and troubleshoot daily. Airflow UI are easy to monitor and troubleshoot the pipeline.We would need to deploy this pipeline as DAG in the Airflow and the DAG page would show us the status of the updates at task level.\n",
    "###### What are the configuration you need to do in identified tool ?\n",
    "Airflow is workflow scheduler for scheduling complex workflows and provide an easy way to maintain them.It does require configuration for defining and scheduling the DAG.\n",
    "Basic configuration steps are:\n",
    "* Identify the tasks to run such as executing python script, sql script or shell script\n",
    "* Idenfity the Operator to run each of those tasks such as Python Operator, Bash Shell Operator.\n",
    "* Configure Scheduler to define when these tasks should run in terms of time and dependency.\n",
    "\n",
    "###### How will this impact business\n",
    "By running the code in workflow, business get benefits in maintaining, versioning, testing and collaborating their code.\n",
    "\n",
    "###### What is the operating cost ?\n",
    "Airflow is an open source tool. I think it is free to use.\n",
    "\n",
    "###### Does the tool require a separate instance ?\n",
    "Airflow requires many instances to be running before we can use them, such as scheduler, webserver and database. I could not set it up on my local machine while working on the Airflow project due to time constraint.\n",
    "\n",
    "##### The database needed to be accessed by 100+ people.\n",
    "We have used Hive enabled local Spark session in this project. Hive metastore in this embedded mode allows only one session to access the derby database. When the number of users increases, metastore can be configured in network mode on a separate machine where different connectors can be used to connect to remote database databases like MySQL, Postgres, Oracle. And also metastore in its remote mode can be configured to connect to the datawarehouse in cloud\n",
    "###### Does replication make sense ?\n",
    "Replication of Hive metastore is possible as per this link https://cwiki.apache.org/confluence/display/Hive/Replication\n",
    "But it does not make much sense to me at this time when I have just worked on embedded database as metastore_db.\n",
    "\n",
    "###### What type of replication ?\n",
    "As written in the above link, the Hive replication system has a low degree of coupling between source cluster and its replica and thrift server is the integration point between them.\n",
    "\n",
    "###### How can your tool accommodate massive influx? What kind of access and what type of configuration?\n",
    "Hive metastore can accomodate massive influx when it is configured in network mode or remote. In the network mode, metastore_db is set up on separate machine where it is configured using jdbc drivers  to connect to RDBMS databases like Postgres, MySQL or even Oracle. In remote mode metastore_db can connect to dataware house in the cloud such as Amazon's Redshift. \n",
    "\n",
    "###### Again the business benefits and impacts on cost\n",
    "Apache Hive being open source software benefits businesses with its ease of usage and free of cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5.4:Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "This project template was provided with a SAS module \"saurfang:spark-sas7bdat:3.0.0-s_2.12\" and two data sets, one in the parquet format and another in the CSV format. After reviewing my past projects submitted in this course and detail reading on the articles on internet, I thought it is correct to implement the pipeline using local spark session enabled with HIVE datawarehouse. Hive can be configured to connect to a local database or a remote database, but in this project spark session uses the embedded derby database. This derby database acts as metastore for the data model tables which have their data located in AWS S3 storage.\n",
    "###### why the chosen schema\n",
    "In this schema or database, meta data is stored in the metastore of Hive and data is located on distribution storage allowing users to run the queries on large data sets.\n",
    "\n",
    "###### is there a trade-off\n",
    "In traditional databases, schema is enforced at data load time.If the data doesn't conform to the schema, it is rejected. This is called schema on write. But in Hive, data is verified when a query is issued known as schema on read.Hive shines over traditional databases in this context because data load time is much lesser than the traditional databases.\n",
    "###### furnish results of the sample queries (Questions) mentioned in scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|        country_city|count(1)|\n",
      "+--------------------+--------+\n",
      "|     UNITED KINGDOM |  368421|\n",
      "|              JAPAN |  249167|\n",
      "|         CHINA, PRC |  185609|\n",
      "|             FRANCE |  185339|\n",
      "|MEXICO Air Sea, a...|  179603|\n",
      "|            GERMANY |  156613|\n",
      "|        SOUTH KOREA |  136312|\n",
      "|             BRAZIL |  134907|\n",
      "|          AUSTRALIA |  112407|\n",
      "|              INDIA |  107193|\n",
      "+--------------------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Top 5 countries visiting the US\n",
    "query1=\"\"\"select country_city, count(*) from country_lookup_export join i94_table_export on i94_table_export.i94res=country_lookup_export.code group by country_city order by 2 desc;\"\"\"  \n",
    "spark.sql(query1).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------------+------------+\n",
      "|visa_code|visa_description|VisitorCount|\n",
      "+---------+----------------+------------+\n",
      "|      1.0|        Business|      522079|\n",
      "|      2.0|        Pleasure|     2530868|\n",
      "|      3.0|         Student|       43366|\n",
      "+---------+----------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Visitors count for different visa types\n",
    "query2=\"\"\"select visa_code, visa_description, case when visa_code=1.0 then count(admnum) when visa_code=2.0 then count(admnum) else count(admnum) end as VisitorCount from i94_table_export left outer join i94_visa_export on i94_visa_export.visa_code=i94_table_export.i94visa group by visa_code, visa_description order by visa_code\"\"\"\n",
    "spark.sql(query2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+--------------+-------------+\n",
      "|state_code|  state|          city|count(admnum)|\n",
      "+----------+-------+--------------+-------------+\n",
      "|        AL|Alabama|    Birmingham|        40940|\n",
      "|        AL|Alabama|        Dothan|        40940|\n",
      "|        AL|Alabama|        Hoover|        32752|\n",
      "|        AL|Alabama|    Huntsville|        40940|\n",
      "|        AL|Alabama|        Mobile|        40940|\n",
      "|        AL|Alabama|    Montgomery|        40940|\n",
      "|        AL|Alabama|    Tuscaloosa|        40940|\n",
      "|        AK| Alaska|     Anchorage|         8020|\n",
      "|        AZ|Arizona|      Avondale|       101090|\n",
      "|        AZ|Arizona|  Casas Adobes|       101090|\n",
      "|        AZ|Arizona|      Chandler|       101090|\n",
      "|        AZ|Arizona|     Flagstaff|       101090|\n",
      "|        AZ|Arizona|       Gilbert|       101090|\n",
      "|        AZ|Arizona|      Glendale|       101090|\n",
      "|        AZ|Arizona|      Goodyear|       101090|\n",
      "|        AZ|Arizona|          Mesa|       101090|\n",
      "|        AZ|Arizona|        Peoria|       101090|\n",
      "|        AZ|Arizona|       Phoenix|       101090|\n",
      "|        AZ|Arizona|San Tan Valley|       101090|\n",
      "|        AZ|Arizona|    Scottsdale|       101090|\n",
      "+----------+-------+--------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Visitor counts in different States and Cities\n",
    "query3=\"\"\"select state_code, state, city, count(admnum) from i94_table_export\n",
    "       left outer join us_cities_export on us_cities_export.state_code=i94_table_export.i94addr\n",
    "where state_code is not null and state is not null\n",
    "group by state_code, state, city\n",
    "order by 2,3\"\"\"\n",
    "spark.sql(query3).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------------+-------+-------------+\n",
      "|code|         city_state|i94port|count(admnum)|\n",
      "+----+-------------------+-------+-------------+\n",
      "|5T6 | No PORT Code (5T6)|    5T6|            4|\n",
      "|ATW | No PORT Code (ATW)|    ATW|            3|\n",
      "|CPX | No PORT Code (CPX)|    CPX|            1|\n",
      "|JFA | No PORT Code (JFA)|    JFA|            3|\n",
      "|JMZ | No PORT Code (JMZ)|    JMZ|            7|\n",
      "|MTH | No PORT Code (MTH)|    MTH|            2|\n",
      "|NC8 | No PORT Code (NC8)|    NC8|            1|\n",
      "|NYL | No PORT Code (NYL)|    NYL|            5|\n",
      "|PHF | No PORT Code (PHF)|    PHF|            1|\n",
      "|RYY | No PORT Code (RYY)|    RYY|            2|\n",
      "|SCH | No PORT Code (SCH)|    SCH|            1|\n",
      "|W55 | No PORT Code (W55)|    W55|           33|\n",
      "|X44 | No PORT Code (X44)|    X44|           11|\n",
      "|X96 | No PORT Code (X96)|    X96|         2378|\n",
      "|YGF | No PORT Code (YGF)|    YGF|         1763|\n",
      "+----+-------------------+-------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Number of visitors having missing port of entries\n",
    "spark.sql(\"select distinct code, city_state, i94port, count(admnum) from i94_table_export left outer join port_city_state_export on trim( port_city_state_export.code)=trim(i94_table_export.i94port) where trim(city_state) like 'No PORT Code%' group by code, city_state, i94port order by 1 ; \").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------+\n",
      "|week|count(admnum)|\n",
      "+----+-------------+\n",
      "|  13|       623150|\n",
      "|  14|      1382374|\n",
      "|  15|      1438628|\n",
      "|  16|      1421666|\n",
      "|  17|      1326808|\n",
      "+----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#weekly iternational arrival to US\n",
    "query5=\"\"\"select week,  count(admnum)  from i94_table_export\n",
    "left outer join i94_time_export on i94_time_export.sas_date=i94_table_export.arrdate\n",
    "group by 1\n",
    "order by 1;\"\"\"\n",
    "spark.sql(query5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
